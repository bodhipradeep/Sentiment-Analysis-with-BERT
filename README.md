# Twitter Sentiment Analysis using BERT | Distilbert

This repository provides an **endâ€‘toâ€‘end notebook** for fineâ€‘tuning a BERT model on a labelled Twitterâ€‘sentiment dataset. Leveraging `ğŸ¤— Transformers`, `PyTorch`, and `scikitâ€‘learn`, the notebook walks you through every stepâ€”from data cleaning to model evaluation and savingâ€”so you can quickly reproduce or extend a productionâ€‘ready sentimentâ€‘analysis pipeline.

> **Notebook**: [`bert_sentiment_test.ipynb`](bert_sentiment_test.ipynb)
> **Web App**: FastAPI-based UI with real-time sentiment prediction.   
> **Trained Model**: [Saved Trained Model Link](https://drive.google.com/drive/folders/1umKLrIgb8kWCyOU1oqx9T_R_ueq04qn0?usp=sharing)  
> **Dataset Source**: [KaggleÂ â€“Â TwitterÂ SentimentÂ Analysis](https://www.kaggle.com/datasets)  

---

## ğŸ’» FastAPI Web Interface
Navigate to http://localhost:8000

Enter a sentence â†’ Predict sentiment in real time

### ğŸ–¼ï¸ Sample Screenshots
Input Example
<p align="center">
<img src="https://github.com/user-attachments/assets/241d2037-fc56-4860-a749-ab91ffbc9ed9" width="45%"/>
&nbsp;&nbsp;&nbsp;
<img src="https://github.com/user-attachments/assets/edf3bd8b-db77-4a3b-8d29-d09122445b41" width="45%"/>
</p>
Output Predictions
<p align="center">
<img src="https://github.com/user-attachments/assets/3dc211af-d600-4727-82f8-31387faed1ad" width="45%"/> 
&nbsp;&nbsp;&nbsp;
<img src="https://github.com/user-attachments/assets/c7b9f916-0b80-4072-b824-7793a094b2f2" width="45%"/>
</p>

---
## ğŸ”§ Key Features

1. **Multiâ€‘class sentiment support** â€“ Predicts **Positive, Neutral, Negative,â€¯and Irrelevant** classes outâ€‘ofâ€‘theâ€‘box.  
2. **Minimal setup** â€“ Single Jupyter notebook; no extra Python scripts required.  
3. **HuggingÂ Face Trainer API** â€“ Uses `Trainer` & `TrainingArguments` for streamlined fineâ€‘tuning.
4. **FastAPI Web Interface** â€“ Lightweight web app for real-time predictions.   
5. **Dockerized Deployment** â€“ Easily build, run, and share using Docker.   
6. **GPUâ€‘ready** â€“ Automatically detects CUDA for faster training on compatible hardware.  
7. **Custom metrics** â€“ Computes accuracy via a pluggable `compute_metrics` callback.  
8. **Model persistence** â€“ Exports both the fineâ€‘tuned model and tokenizer with `save_pretrained()` for later inference.  

---

## ğŸ§­ Pipeline Flow

1. **DataÂ Load** â€“ Reads `twitter_training.csv` (â‰ˆâ€‹75â€¯k rows).  
2. **Preâ€‘processing** â€“ Cleans text, maps labels to integers, drops N/A rows.  
3. **TrainÂ â„Â Test Split** â€“ Uses `train_test_split` (default 80â€¯/â€¯20).  
4. **Tokenisation** â€“ BPE tokenisation with `bertâ€‘baseâ€‘uncased`.  
5. **Dataset Wrappers** â€“ Converts to `torch.utils.data.Dataset` objects.  
6. **Fineâ€‘tuning** â€“ Optimises for 2â€“3Â epochs with AdamW on a single GPU/CPU.  
7. **Evaluation** â€“ Reports accuracy on holdâ€‘out test set (â‰ˆâ€¯90â€¯% on sample run).  
8. **Model Saving** â€“ WritesÂ & tokenizer to `./sentimentâ€‘bert/`.
9. **FastAPI App** â€“ Load model/tokenizer and serve predictions via web form.
10. **Dockerized Deployment** â€“ Build and run with simple Docker commands.   

---

## ğŸ“ˆ Sample Output

***** Eval results *****   
epoch = 3   
eval_accuracy = 0.9012   
eval_loss = 0.2904   
eval_runtime = 0:00:18.43   
eval_samples_per_second = 815.4   
eval_steps_per_second = 51.0   

---

## ğŸ“‚ ProjectÂ Structure
```bash
â”œâ”€â”€ bert_sentiment.ipynb   # Endâ€‘toâ€‘end BERT fineâ€‘tuning workflow
â”œâ”€â”€ requirements.txt       # Python package list
â”œâ”€â”€ README.md              # This file
â””â”€â”€ sentimentâ€‘bert/        # (Created after training) saved model & tokenizer
```
---

## ğŸ“„ License

This project is licensed under the [MIT License](../LICENSE).

--- 

## ğŸ”— **Links & Contact**

- **GitHub Profile:** [Github](https://github.com/pradeep-kumar8/)
- **LinkedIn:** [Likedin](https://linkedin.com/in/pradeep-kumar8)
- **Email:** [gmail](mailto:pradeep.kmr.pro@gmail.com)
